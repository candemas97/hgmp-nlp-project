{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me llamo\n",
      "Bum Bum\n"
     ]
    }
   ],
   "source": [
    "n_palabras_query = 2\n",
    "tipos_query = [\"Artista y Letra\",\"Letra\"]\n",
    "tipo_query = tipos_query[0]\n",
    "\n",
    "canciones = [\"Hola yo me llamo pepito\",\"Bum Bum la rumba se prende\"]\n",
    "\n",
    "for c in canciones:\n",
    "    ## TEMPORAL ##\n",
    "    tokens = c.split(\" \")\n",
    "    ## TEMPORAL ##\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text\n",
    "import sklearn.metrics.pairwise\n",
    "import pandas\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "import simplemma\n",
    "import random\n",
    "model_es = spacy.load(\"es_core_news_sm\")\n",
    "def reemplazar_tildes(texto: str) -> str:\n",
    "    reemplazos = {\"á\": \"a\", \"é\": \"e\", \"í\": \"i\", \"ó\": \"o\", \"ú\": \"u\"}\n",
    "    for original, reemplazo in reemplazos.items():\n",
    "        texto = texto.replace(original, reemplazo)\n",
    "    return texto\n",
    "def normalize_text(text: str) -> str:\n",
    "    doc = model_es(\" \".join(simplemma.text_lemmatizer(text, lang=\"es\")))\n",
    "    return \" \".join([reemplazar_tildes(str(token)) for token in doc if token.is_alpha and not token.is_stop])\n",
    "df_normalize = pandas.read_csv(\"./data_normalize.csv\")\n",
    "corpus = df_normalize[\"normalize\"].to_list()\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "def process_query(query: str) -> str:\n",
    "    return normalize_text(query)\n",
    "def get_query_vector(processed_query: str) -> np.ndarray:\n",
    "    query_vector = vectorizer.transform([processed_query]).toarray()\n",
    "    return query_vector\n",
    "def get_song_recommendations(query_vector: np.ndarray, top_n=5) -> pandas.DataFrame:\n",
    "    cosine_similarities = sklearn.metrics.pairwise.cosine_similarity(query_vector, X).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[: -top_n - 1 : -1]\n",
    "    return df_normalize.iloc[related_docs_indices]\n",
    "def search_and_recommend(query: str, top_n=5) -> pandas.DataFrame:\n",
    "    processed_query = process_query(query)\n",
    "    query_vector = get_query_vector(processed_query)\n",
    "    recommendations = get_song_recommendations(query_vector, top_n)\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR - WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hbeat\\Desktop\\Maestría (local)\\2. Semestre II\\NLP\\hgmp-nlp-project\\entregas\\entrega1\\evaluacion\\evaluacion.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hbeat/Desktop/Maestr%C3%ADa%20%28local%29/2.%20Semestre%20II/NLP/hgmp-nlp-project/entregas/entrega1/evaluacion/evaluacion.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hbeat/Desktop/Maestr%C3%ADa%20%28local%29/2.%20Semestre%20II/NLP/hgmp-nlp-project/entregas/entrega1/evaluacion/evaluacion.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39;49mdownload(\u001b[39m\"\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m\"\u001b[39;49m, quiet\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hbeat/Desktop/Maestr%C3%ADa%20%28local%29/2.%20Semestre%20II/NLP/hgmp-nlp-project/entregas/entrega1/evaluacion/evaluacion.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hbeat/Desktop/Maestr%C3%ADa%20%28local%29/2.%20Semestre%20II/NLP/hgmp-nlp-project/entregas/entrega1/evaluacion/evaluacion.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\site-packages\\nltk\\downloader.py:777\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(s, prefix2\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    769\u001b[0m     print_to(\n\u001b[0;32m    770\u001b[0m         textwrap\u001b[39m.\u001b[39mfill(\n\u001b[0;32m    771\u001b[0m             s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m         )\n\u001b[0;32m    775\u001b[0m     )\n\u001b[1;32m--> 777\u001b[0m \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincr_download(info_or_id, download_dir, force):\n\u001b[0;32m    778\u001b[0m     \u001b[39m# Error messages\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(msg, ErrorMessage):\n\u001b[0;32m    780\u001b[0m         show(msg\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\site-packages\\nltk\\downloader.py:637\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(info, Collection):\n\u001b[0;32m    636\u001b[0m     \u001b[39myield\u001b[39;00m StartCollectionMessage(info)\n\u001b[1;32m--> 637\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincr_download(info\u001b[39m.\u001b[39mchildren, download_dir, force)\n\u001b[0;32m    638\u001b[0m     \u001b[39myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[0;32m    640\u001b[0m \u001b[39m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\site-packages\\nltk\\downloader.py:624\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[39m# If they gave us a list of ids, then download each one.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(info_or_id, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 624\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_list(info_or_id, download_dir, force)\n\u001b[0;32m    625\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[39m# Look up the requested collection or package.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\site-packages\\nltk\\downloader.py:667\u001b[0m, in \u001b[0;36mDownloader._download_list\u001b[1;34m(self, items, download_dir, force)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    666\u001b[0m     delta \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(item\u001b[39m.\u001b[39mpackages) \u001b[39m/\u001b[39m num_packages\n\u001b[1;32m--> 667\u001b[0m \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincr_download(item, download_dir, force):\n\u001b[0;32m    668\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(msg, ProgressMessage):\n\u001b[0;32m    669\u001b[0m         \u001b[39myield\u001b[39;00m ProgressMessage(progress \u001b[39m+\u001b[39m msg\u001b[39m.\u001b[39mprogress \u001b[39m*\u001b[39m delta)\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\site-packages\\nltk\\downloader.py:642\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[39myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[0;32m    640\u001b[0m \u001b[39m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 642\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_package(info, download_dir, force)\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\site-packages\\nltk\\downloader.py:735\u001b[0m, in \u001b[0;36mDownloader._download_package\u001b[1;34m(self, info, download_dir, force)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39munzip \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(zipdir, info\u001b[39m.\u001b[39mid)):\n\u001b[0;32m    734\u001b[0m     \u001b[39myield\u001b[39;00m StartUnzipMessage(info)\n\u001b[1;32m--> 735\u001b[0m     \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m _unzip_iter(filepath, zipdir, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    736\u001b[0m         \u001b[39m# Somewhat of a hack, but we need a proper package reference\u001b[39;00m\n\u001b[0;32m    737\u001b[0m         msg\u001b[39m.\u001b[39mpackage \u001b[39m=\u001b[39m info\n\u001b[0;32m    738\u001b[0m         \u001b[39myield\u001b[39;00m msg\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\site-packages\\nltk\\downloader.py:2254\u001b[0m, in \u001b[0;36m_unzip_iter\u001b[1;34m(filename, root, verbose)\u001b[0m\n\u001b[0;32m   2251\u001b[0m     \u001b[39myield\u001b[39;00m ErrorMessage(filename, e)\n\u001b[0;32m   2252\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m-> 2254\u001b[0m zf\u001b[39m.\u001b[39;49mextractall(root)\n\u001b[0;32m   2256\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[0;32m   2257\u001b[0m     \u001b[39mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\zipfile.py:1681\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(path)\n\u001b[0;32m   1680\u001b[0m \u001b[39mfor\u001b[39;00m zipinfo \u001b[39min\u001b[39;00m members:\n\u001b[1;32m-> 1681\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(zipinfo, path, pwd)\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\zipfile.py:1736\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1732\u001b[0m     \u001b[39mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(member, pwd\u001b[39m=\u001b[39mpwd) \u001b[39mas\u001b[39;00m source, \\\n\u001b[0;32m   1735\u001b[0m      \u001b[39mopen\u001b[39m(targetpath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m target:\n\u001b[1;32m-> 1736\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopyfileobj(source, target)\n\u001b[0;32m   1738\u001b[0m \u001b[39mreturn\u001b[39;00m targetpath\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\shutil.py:197\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m    195\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[0;32m    196\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[0;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[0;32m    199\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\zipfile.py:955\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    954\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[1;32m--> 955\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[0;32m    957\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\zipfile.py:1045\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m-> 1045\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_crc(data)\n\u001b[0;32m   1046\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\hbeat\\miniconda3\\envs\\nlp\\Lib\\zipfile.py:970\u001b[0m, in \u001b[0;36mZipExtFile._update_crc\u001b[1;34m(self, newdata)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    968\u001b[0m     \u001b[39m# No need to compute the CRC if we don't have a reference value\u001b[39;00m\n\u001b[0;32m    969\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m=\u001b[39m crc32(newdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc)\n\u001b[0;32m    971\u001b[0m \u001b[39m# Check the CRC if we're at the end of the file\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\", quiet=True)\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "wordvectors_file_vec = \"/embeddings-l-model.vec\"\n",
    "# cantidad = 100000\n",
    "model = KeyedVectors.load_word2vec_format(wordvectors_file_vec)  # , limit=cantidad)\n",
    "ruta = \"../../../data/data.csv\"\n",
    "data = pd.read_csv(ruta)\n",
    "data[\"unido_todo\"] = (\n",
    "    ((data[\"Artista\"] + \" \") * 5)\n",
    "    + ((data[\"Titulo\"] + \" \") * 5)\n",
    "    + data[\"Cancion\"]  # Se multiplica por 5 para darle un mayor peso a artista y titulo\n",
    ")\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "spanishstemmer = SnowballStemmer(\"spanish\")\n",
    "def preprocesamiento_stemas(text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Función encargada de prepocesar la data de las canciones\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stema = [spanishstemmer.stem(w) for w in tokens]  # devuelve palabras stemizadas\n",
    "    return \" \".join(stema)\n",
    "X = data[\"unido_todo\"].apply(preprocesamiento_stemas)\n",
    "X.head()\n",
    "def embeddings(word: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Función encargada de realizar los embeddings de las palabras\n",
    "    \"\"\"\n",
    "    if word in model.key_to_index:\n",
    "        return model.get_vector(word)\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "diccionario = {}\n",
    "for idx, fila in enumerate(X):\n",
    "    average_vector = np.mean(\n",
    "        np.array([embeddings(palabra) for palabra in fila.split()]), axis=0\n",
    "    )\n",
    "    d1 = {idx: (average_vector)}\n",
    "    diccionario.update(d1)\n",
    "def similaridad(query_embedding: np.array, average_vec: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calcular la similitud del coseno\n",
    "    \"\"\"\n",
    "    sim = [(1 - scipy.spatial.distance.cosine(query_embedding, average_vec))]\n",
    "    return sim\n",
    "def seleccion_canciones_word2vec(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Función que calcula distancias entre query y canciones.\n",
    "    \"\"\"\n",
    "    query_words = np.mean(\n",
    "        np.array(\n",
    "            [embeddings(palabra) for palabra in preprocesamiento_stemas(query).split()],\n",
    "            dtype=float,\n",
    "        ),\n",
    "        axis=0,\n",
    "    )\n",
    "    rank = []\n",
    "    for k, v in diccionario.items():\n",
    "        rank.append((k, similaridad(query_words, v)))  # data['Titulo'][k]\n",
    "    rank = sorted(rank, key=lambda t: t[1], reverse=True)\n",
    "    print(\"Canciones relacionadas: \")\n",
    "    return rank[:20]\n",
    "def main(texto: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Función principal para encontrar la canción solicitada\n",
    "    \"\"\"\n",
    "    canciones_respuesta = seleccion_canciones_word2vec(texto)\n",
    "    diccionario_resultado = {\"Titulo\": [], \"Artista\": [], \"Simularidad\": []}\n",
    "    for cancion in canciones_respuesta:\n",
    "        diccionario_resultado[\"Titulo\"].append(data[\"Titulo\"][cancion[0]])\n",
    "        diccionario_resultado[\"Artista\"].append(data[\"Artista\"][cancion[0]])\n",
    "        diccionario_resultado[\"Simularidad\"].append(cancion[1])\n",
    "    return pd.DataFrame(diccionario_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplemma import simple_tokenizer\n",
    "def get_tokens(text: str) -> str:\n",
    "    doc = model_es(\" \".join(simple_tokenizer(text)))\n",
    "    return [reemplazar_tildes(str(token)) for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "def get_random_query(cancion,n_palabras_query=5):\n",
    "    tokens = get_tokens(cancion)\n",
    "    longitud_cancion = len(tokens)\n",
    "    try:\n",
    "        ultimo_token_query = max(longitud_cancion - n_palabras_query,1)\n",
    "        # print(ultimo_token_query)\n",
    "        pos_query = np.random.randint(0,ultimo_token_query)\n",
    "        # print(pos_query)\n",
    "        query = \" \".join(tokens[pos_query:min(pos_query+n_palabras_query,len(tokens))])\n",
    "        return query\n",
    "    except:\n",
    "        print('Exception')\n",
    "        return \"lasdlasd\"\n",
    "  \n",
    "def resultado_prediccion(row,top_n=5):\n",
    "    letra_cancion = row['Cancion']\n",
    "    target_idx = row.name\n",
    "    query = get_random_query(letra_cancion)\n",
    "    predicted_idxs = search_and_recommend(query,top_n=top_n).index.tolist()\n",
    "    return int(target_idx in predicted_idxs), target_idx, predicted_idxs, query\n",
    "\n",
    "def evaluar(df,tamaño_muestra=100):\n",
    "    sample_idxs = np.random.choice(df.index.tolist(),size=tamaño_muestra)\n",
    "    df_eval = df.loc[sample_idxs,:]\n",
    "    info = df_eval.apply(resultado_prediccion,top_n=5,axis=1)\n",
    "    res = np.array([x for x,y,z,a in info])\n",
    "    target_idx = [y for x,y,z,a in info]\n",
    "    predicted_idxs = [z for x,y,z,a in info]\n",
    "    query = [a for x,y,z,a in info]\n",
    "    return res.mean(), res, target_idx, predicted_idxs, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1d-E2d-E3d-E4d-E5d-E6d-E7d-E8d-E9d-E10d-E11d-E12d-E13d-E14d-E15d-E16d-E17d-E18d-E19d-E20d-\n",
      "Accuracy Promedio = 0.6478\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "resultados = []\n",
    "ti = []\n",
    "pi = []\n",
    "q = []\n",
    "for i in range(20):\n",
    "    acc, res, target_idx, predicted_idxs, query = evaluar(df_normalize,tamaño_muestra=200)\n",
    "    accs.append(acc)\n",
    "    resultados.append(res)\n",
    "    ti.append(target_idx)\n",
    "    pi.append(predicted_idxs)\n",
    "    q.append(query)\n",
    "    \n",
    "    print(\"E\"+str(i+1)+\"d-\",end=\"\")\n",
    "print(\"\")\n",
    "acc_mean = np.array(accs).mean()\n",
    "print(\"Accuracy Promedio = \"+str(np.round(acc_mean,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n",
      "[2184, 634, 2265, 1139, 623]\n",
      "vida mundo acabara imposible existir\n"
     ]
    }
   ],
   "source": [
    "ej = 10\n",
    "print(ti[0][ej])\n",
    "print(pi[0][ej])\n",
    "print(q[0][ej])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
